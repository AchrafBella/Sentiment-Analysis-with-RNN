{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Recurrent Neural Network & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all i would like to thank Laboratoire d'Informatique en Image et Systèmes d'Information (LIRIS) in collaboration with the Ecole Centrale de Lyon that give me the opportunity to achieve this work in the best environnement that helped me to improve my knowledge and to sharpen my skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectif\n",
    "In this notebook we are going through all the basic and necessary knowledge to understand how we can apply both machine learning and deep learning approach in sentiment analysis, we are looking to understand how all the functionalities and what's behind going to work, to do so we are going to code everything from the scratch. after getting a good understanding we’ll walk through concrete code examples and a full Tensorflow sentiment classifier at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Part 1** Recurent neural netowrk\n",
    "- **Part 2** Words to vectors\n",
    "- **Part 3** Pre-processing data\n",
    "- **Part 4** Application using Keras\n",
    "- **Part 5** Logstic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Recurent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurent neural network is a type of neural network quite complicated compared to simple neural network in term of how we process forward propagation and backward propagation. We are using RNN because of the dependency because we are going to process speech "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<td>\n",
    "<img src=\"images/RNN.png\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between simple neural network (NN) and recurent neural network (RNN) is that RNN at took each time an input with a diffrent lenght to process it, for exemple we have a text like **\"hello, world\"** and **\"recurrent neural network\"** in this case our lenght is 2 and 3 respectively. \n",
    "\n",
    "Let's simulate how a RNN will process our input x which equals to \"hello, world\". RNN will process it 2 times because of the lenght of our input as follows x(t=0) = 'hello' then x(t=1) = 'world' using some transformation to convert words into numerical values. another one important characteristic is that RNN use the previous data to process the next one in our case RNN will use the data from x(t=0) to process x(t=1) with the help of a hidden state and understand that hello always followed by world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<td>\n",
    "<img src=\"images/RNN_equations.png\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the formula above we can implement our forword propagation and then the only thing we need is to update the wieghts which is the connection between the layers to get the accurat resultat using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<td>\n",
    "<img src=\"images/backpro.png\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : words to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand how we use the natural language in machine learning let's took a look at other types of neural networks Convolutional neural networks use arrays of pixel values, logistic regression uses quantifiable features, and reinforcement learning models use reward signals. The common theme is that the inputs need to be scalar. Lets see how we can do this using words to vectors and some similarity algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to give a text as an input to our RNN and give back in return a state which is either positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<td>\n",
    "<img src=\"images/S.png\"style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find a way to code every single word into an array and give the collection of the array as an input like the figure below excatly show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src = \"images/S2.png\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, there is two way to code texts into vectors\n",
    "* The first method consist of importing a corpus full of words indexed from 0 to the last word, we are going to give and input as an array where all the element are 0 expect the index of the word in the corpus where it's 1 \n",
    "* The second methode named word2vec and like the name applies it turn words into vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple application to the first exemple we are going to code words to predict the next letter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'bad', 'this', 'not', 'and', 'earlier', 'at', 'or', 'all', 'now', 'good', 'i', 'right', 'was', 'happy', 'very', 'am', 'sad']\n",
      "Nous avons  18  mot unique \n",
      "[array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0.])]\n"
     ]
    }
   ],
   "source": [
    "from data import train_data\n",
    "import numpy as np\n",
    "vocabulaire = [w for text in train_data.keys() for w in text.split(' ')]\n",
    "vocabulaire = list(set(vocabulaire))\n",
    "\n",
    "print(vocabulaire)\n",
    "\n",
    "print(\"Nous avons \", len(vocabulaire),\" mot unique \")\n",
    "\n",
    "word_to_id = { w: i for i, w in enumerate(vocabulaire) }\n",
    "id_to_word = { i: w for i, w in enumerate(vocabulaire) }\n",
    "\n",
    "text = 'this is very good'.split(' ')\n",
    "X = list()\n",
    "for elm in text:\n",
    "    x = np.zeros(18)\n",
    "    x[word_to_id[elm]] = 1\n",
    "    X.append(x)\n",
    "    pass\n",
    "pass\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src = \"images/S3.png\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "    \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "love = word_to_vec_map['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adore = word_to_vec_map['adore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can mesure how two word similar Cosine similarity this method consist of compute the ongle between two vector let's see how we can do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<td>\n",
    "<img src = \"images/s4.png\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u.v = cos(u.v) * |u|*|v|\n",
    "# the similarity is to caclulate the consinus\n",
    "import numpy as np\n",
    "def smilarity(U, V):\n",
    "    UV = np.dot(U,V)\n",
    "    norme_U = np.sqrt(np.sum(U*U)) \n",
    "    norme_V = np.sqrt(np.sum(V*V))\n",
    "    return UV/(norme_U * norme_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now let's see how the two word LOVE & ADORE similar.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42786951433899845"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smilarity(love, adore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see the function smilarity give as the smilarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, We'll be using a dataset from the Kaggle This dataset is already located in the folder for this section.\n",
    "\n",
    "The file we are using contains a collection of more than 5 thousand comment and opnion of movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data_cleaned'] = data.review.apply(lambda x: clean_text_round1(x))\n",
    "data['data_cleaned'] = data['data_cleaned'].apply(lambda x: clean_text_round1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data_tokens'] = data['data_cleaned'].apply(lambda x : x.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>data_cleaned</th>\n",
       "      <th>data_tokens</th>\n",
       "      <th>data_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>[one, of, the, other, reviewer, ha, mentioned,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a wonderful little production br br the filmin...</td>\n",
       "      <td>[a, wonderful, little, production, br, br, the...</td>\n",
       "      <td>[a, wonderful, little, production, br, br, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>[i, thought, this, wa, a, wonderful, way, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>[basically, theres, a, family, where, a, littl...</td>\n",
       "      <td>[basically, there, a, family, where, a, little...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>[petter, matteis, love, in, the, time, of, mon...</td>\n",
       "      <td>[petter, matteis, love, in, the, time, of, mon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                        data_cleaned  \\\n",
       "0  one of the other reviewers has mentioned that ...   \n",
       "1  a wonderful little production br br the filmin...   \n",
       "2  i thought this was a wonderful way to spend ti...   \n",
       "3  basically theres a family where a little boy j...   \n",
       "4  petter matteis love in the time of money is a ...   \n",
       "\n",
       "                                         data_tokens  \\\n",
       "0  [one, of, the, other, reviewers, has, mentione...   \n",
       "1  [a, wonderful, little, production, br, br, the...   \n",
       "2  [i, thought, this, was, a, wonderful, way, to,...   \n",
       "3  [basically, theres, a, family, where, a, littl...   \n",
       "4  [petter, matteis, love, in, the, time, of, mon...   \n",
       "\n",
       "                                     data_lemmatized  \n",
       "0  [one, of, the, other, reviewer, ha, mentioned,...  \n",
       "1  [a, wonderful, little, production, br, br, the...  \n",
       "2  [i, thought, this, wa, a, wonderful, way, to, ...  \n",
       "3  [basically, there, a, family, where, a, little...  \n",
       "4  [petter, matteis, love, in, the, time, of, mon...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "data['data_lemmatized'] = data.data_tokens.apply(lambda x : lemmatize_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>data_cleaned</th>\n",
       "      <th>data_tokens</th>\n",
       "      <th>data_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "      <td>49581</td>\n",
       "      <td>49581</td>\n",
       "      <td>49581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "      <td>loved todays show it was a variety and not sol...</td>\n",
       "      <td>[loved, todays, show, it, was, a, variety, and...</td>\n",
       "      <td>[loved, today, show, it, wa, a, variety, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment  \\\n",
       "count                                               50000     50000   \n",
       "unique                                              49582         2   \n",
       "top     Loved today's show!!! It was a variety and not...  positive   \n",
       "freq                                                    5     25000   \n",
       "\n",
       "                                             data_cleaned  \\\n",
       "count                                               50000   \n",
       "unique                                              49581   \n",
       "top     loved todays show it was a variety and not sol...   \n",
       "freq                                                    5   \n",
       "\n",
       "                                              data_tokens  \\\n",
       "count                                               50000   \n",
       "unique                                              49581   \n",
       "top     [loved, todays, show, it, was, a, variety, and...   \n",
       "freq                                                    5   \n",
       "\n",
       "                                          data_lemmatized  \n",
       "count                                               50000  \n",
       "unique                                              49581  \n",
       "top     [loved, today, show, it, wa, a, variety, and, ...  \n",
       "freq                                                    5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">review</th>\n",
       "      <th colspan=\"4\" halign=\"left\">data_cleaned</th>\n",
       "      <th colspan=\"4\" halign=\"left\">data_tokens</th>\n",
       "      <th colspan=\"4\" halign=\"left\">data_lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>25000</td>\n",
       "      <td>24698</td>\n",
       "      <td>You do realize that you've been watching the E...</td>\n",
       "      <td>3</td>\n",
       "      <td>25000</td>\n",
       "      <td>24697</td>\n",
       "      <td>you do realize that youve been watching the ex...</td>\n",
       "      <td>3</td>\n",
       "      <td>25000</td>\n",
       "      <td>24697</td>\n",
       "      <td>[i, see, that, c, thomas, howell, has, appeare...</td>\n",
       "      <td>3</td>\n",
       "      <td>25000</td>\n",
       "      <td>24697</td>\n",
       "      <td>[i, see, that, c, thomas, howell, ha, appeared...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>25000</td>\n",
       "      <td>24884</td>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "      <td>24884</td>\n",
       "      <td>loved todays show it was a variety and not sol...</td>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "      <td>24884</td>\n",
       "      <td>[loved, todays, show, it, was, a, variety, and...</td>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "      <td>24884</td>\n",
       "      <td>[loved, today, show, it, wa, a, variety, and, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          review                                                            \\\n",
       "           count unique                                                top   \n",
       "sentiment                                                                    \n",
       "negative   25000  24698  You do realize that you've been watching the E...   \n",
       "positive   25000  24884  Loved today's show!!! It was a variety and not...   \n",
       "\n",
       "               data_cleaned         \\\n",
       "          freq        count unique   \n",
       "sentiment                            \n",
       "negative     3        25000  24697   \n",
       "positive     5        25000  24884   \n",
       "\n",
       "                                                                  data_tokens  \\\n",
       "                                                         top freq       count   \n",
       "sentiment                                                                       \n",
       "negative   you do realize that youve been watching the ex...    3       25000   \n",
       "positive   loved todays show it was a variety and not sol...    5       25000   \n",
       "\n",
       "                                                                          \\\n",
       "          unique                                                top freq   \n",
       "sentiment                                                                  \n",
       "negative   24697  [i, see, that, c, thomas, howell, has, appeare...    3   \n",
       "positive   24884  [loved, todays, show, it, was, a, variety, and...    5   \n",
       "\n",
       "          data_lemmatized         \\\n",
       "                    count unique   \n",
       "sentiment                          \n",
       "negative            25000  24697   \n",
       "positive            25000  24884   \n",
       "\n",
       "                                                                   \n",
       "                                                         top freq  \n",
       "sentiment                                                          \n",
       "negative   [i, see, that, c, thomas, howell, ha, appeared...    3  \n",
       "positive   [loved, today, show, it, wa, a, variety, and, ...    5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('sentiment').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>data_cleaned</th>\n",
       "      <th>data_tokens</th>\n",
       "      <th>data_lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           review  data_cleaned  data_tokens  data_lemmatized\n",
       "sentiment                                                    \n",
       "negative    25000         25000        25000            25000\n",
       "positive    25000         25000        25000            25000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length'] = data.data_lemmatized.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8,5))\n",
    "plt.hist(data.length, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 576x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8,15))\n",
    "data.hist(column='length', by='sentiment', bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "data['polarity']     = data.data_cleaned.apply(pol)\n",
    "data['subjectivity'] = data.data_cleaned.apply(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = LabelEncoder()\n",
    "data['sentiment'] = encode.fit_transform(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(5000)\n",
    "\n",
    "tokenizer.fit_on_texts(data.data_cleaned)\n",
    "\n",
    "data['data_inputs'] = tokenizer.texts_to_sequences(data.data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['data_inputs'], data.sentiment, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "max_words = 500\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "vocabulary_size = 5000\n",
    "embedding_size=32\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "175/175 [==============================] - 240s 1s/step - loss: 0.6068 - accuracy: 0.6634\n",
      "Epoch 2/8\n",
      "175/175 [==============================] - 268s 2s/step - loss: 0.2833 - accuracy: 0.8884\n",
      "Epoch 3/8\n",
      "175/175 [==============================] - 264s 2s/step - loss: 0.2294 - accuracy: 0.9140\n",
      "Epoch 4/8\n",
      "175/175 [==============================] - 265s 2s/step - loss: 0.2092 - accuracy: 0.9204\n",
      "Epoch 5/8\n",
      "175/175 [==============================] - 265s 2s/step - loss: 0.1952 - accuracy: 0.9279\n",
      "Epoch 6/8\n",
      "175/175 [==============================] - 276s 2s/step - loss: 0.1916 - accuracy: 0.9290\n",
      "Epoch 7/8\n",
      "175/175 [==============================] - 287s 2s/step - loss: 0.1666 - accuracy: 0.9382\n",
      "Epoch 8/8\n",
      "175/175 [==============================] - 278s 2s/step - loss: 0.1636 - accuracy: 0.9390\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "num_epochs = 8\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = [1 if elm>0.5 else 0 for elm in all_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88      7535\n",
      "           1       0.88      0.87      0.88      7465\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great job "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***bibliography***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Operations%20on%20word%20vectors%20-%20v2.ipynb\n",
    "* https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469\n",
    "* https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Dinosaurus%20Island%20--%20Character%20level%20language%20model%20final%20-%20v3.ipynb\n",
    "* https://github.com/adeshpande3/LSTM-Sentiment-Analysis/blob/master/Oriole%20LSTM.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
